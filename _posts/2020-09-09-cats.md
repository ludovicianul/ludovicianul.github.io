---
layout: post
title: How to write self-healing functional API tests with no coding effort
---

APIs are everywhere, and it's critical to have ways to efficiently test that they work correctly.
 
When thinking about testing APIs, from a functional perspective, there are several dimensions which come to mind:
- verifying the structure of the exchanged data i.e. **structural validations** (length, data type, patterns, etc)
- **boundary testing and negative scenarios**, similar a bit to the above, but focusing on "breaking" the application
- behaviour according to documentation i.e. APIs are **responding as expected** in terms of HTTP status codes or response payloads
- functional scenarios i.e. APIS are **working as expected** in terms of expected business behaviour
- linked functional scenarios i.e. you create an entity and you get its details after to check they are the same

# What are the options
There are several frameworks and tools on the market which can help to automate all these. Just to name a few: 
- [Rest Assured](https://rest-assured.io/)
- [Retrofit](https://square.github.io/retrofit/)
- [Postman](https://www.postman.com/)
- [SoapUI](https://www.soapui.org/)

They are all great tools and frameworks. You start writing test cases for the above categories. The tools/frameworks will provide different sets of facilities which might reduce specific effort during implementation, but ultimately you end up writing the actual tests to be executed (i.e. code). 
Even if you've done this before, and you know exactly what to test, even if you create a mini-framework that provides facilities for the common elements, you still need to write a considerable amount of code in order to automate all your testing scenarios.

And what happens when the API changes a bit? Some fields might be more restrictive in terms of validation, some might change the type, some might get renamed and so on. And then a bit more?
This is usually not very rewarding work. Software engineers are usually creative creatures, and they like challenges and solving problems, not doing boring stuff. 
There are cases when one might choose to leave the API as is in order to prevent changing too many test cases.

# Is there a better way?
But what if there is an alternative to this, and the first 3 categories above can be fully automated, including the **actual writing** of the test case. 
And also make the next 2 categories extremely simple to write and maintain. This is the reason I wrote [CATs](https://github.com/Endava/cats).

[CATs](https://github.com/Endava/cats) has 3 main goals in mind:
- **remove the boring activities** when testing APIs by automating the entire testing lifecycle: write, execute and report of test cases
- **auto-heal** when APIs change
- make writing **functional scenarios** as **simple** as possible by entirely removing the need to write code, while still leveraging the first 2 points

There is one catch though. Your API must provide and [OpenAPI](https://swagger.io/specification/) contract/spec. But this shouldn't be an issue in my opinion. It's good practice having your API documented in a tools-friendly format. 
Many companies are building [OpenAPI specs for their API](https://github.blog/2020-07-27-introducing-githubs-openapi-description/) for easier tools integration.

# Using CATs
Let's take an example to show exactly how `CATs` works. I've chosen the [Pet-Store](https://github.com/OpenAPITools/openapi-petstore) application from the OpenAPITools example section.
The OpenAPI spec is available in the same repo: [https://github.com/OpenAPITools/openapi-petstore/blob/master/src/main/resources/openapi.yaml](https://github.com/OpenAPITools/openapi-petstore/blob/master/src/main/resources/openapi.yaml).

Looking at the contract, how much time would you estimate would take to create an automation suite to properly test this API? 1 Day? 2 Days? Several days?
Using `CATs` this will probably be a couple of hours. 

Let's start the pet-store application on the local box first (you need to have [Docker](https://www.docker.com/) installed):
```bash
docker pull openapitools/openapi-petstore
docker run -d -e OPENAPI_BASE_PATH=/v3 -p 80:8080 openapitools/openapi-petstore
```

This will make the app available at `http://localhost` for the Swagger UI, and the API will be available at `http://localhost/v3`.

Download the [latest CATs version](https://github.com/Endava/cats/releases). Download also the [openapi.yml](https://github.com/OpenAPITools/openapi-petstore/blob/master/src/main/resources/openapi.yaml) from above.

Before running `CATs`, please [read how it works](https://github.com/Endava/cats#how-the-fuzzing-works) in order to better interpret the results.

## Running the built-in test cases
Suppose both `cats.jar` and the `openapi.yml` are in the same folder, you can now run:

```bash
./cats.jar --server=http://localhost/v3 --contract=openapi.yml
```

You will get something like this:
![first_run.png](https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/first_run.png)

Not bad! We just generated `437` test cases, out of which `78` were already successful. To view the entire report, just open `test-report/index.html`. You will see something like this:

![test_report_first_run.png](https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/test_report_first_run.png)

Now let's check if the reported `errors` are actually bugs or just configuration issues. We will deal with the `warnings` after.
Checking the first failing test we can see that the reason for failing is actually due to the fact that we didn't send any form of authentication along with the requests.
Looking in the `openapi.yml` we can see that some of the endpoints require authentication while some others require an `api_key`. 

![auth_required.png](https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/auth_required.png)

`CATs` supports [passing any type of headers with the requests](https://github.com/Endava/cats#headers-file).
Let's create the following `headers.yml` file:

```yaml
all:
   Authorization: Bearer 5cc40680-4b7d-4b81-87db-fd9e750b060b
   api_key: special-key
```

You can obtain the `Bearer` token by authenticating in the Swagger UI. The `api_key` has a fixed value as stated in the [pet-store documentation](https://github.com/OpenAPITools/openapi-petstore).

Let's do another run, including now the `headers.yml` file:

```bash
./cats.jar --contract=openapu.yml --server=http://localhost/v3 --headers=headers.yml
```

A bit better now. `93` successful and `140` errors. 

![second_run.png](https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/second_run.png)

Again, let's check if there are any configuration issues, or they are valid `errors`. Looking at `test 1253` we can see that the target endpoint has a parameter inside the url. 

![test_1253.png](https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/test_1253.png)

This is a limitation of `CATs` as it cannot both fuzz the URL and the payloads for `POST, PUT and PATCH` requests. And looking at the contract, indeed, there are several other endpoints that are in the same situation.
For this particular cases `CATs` uses the `urlParams` argument in order to pass some fixed values for these parameters. The provided values must result in existing HTTP resources so that the fuzzing works as expected.

We run `CATs` again, considering also the `urlParams`: 

```bash
./cats.jar r --contract=openapi.yml --server=http://localhost/v3 --headers=headers.yml --urlParams="username:username,petId:10,orderId=1"
```

Even better now. "Just" `81` errors. 

![third_run.png](https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/third_run.png)

Let's check again if these are actual `errors`. This time, they all seem to be valid bugs:
- `Test 429`: the server returns a `500 Internal Error` instead of a normal `200` empty response
- `Test 520`: `CATs` injects a new field inside the request, but the server still responds with `200` instead of returning a `400 Bad Request`
- `Test 523`: `CATs` sends a null value in a non-required field (according to the contract) and expects a `2xx` response, while the server replies with `500 evend ID cannot be null`
- and so on...

They must be actioned and fixed!

Now, going into the `warnings` zone, these are usually tests which fail "just a bit". The server usually responds withing the expected HTTP response code family, but either the HTTP status code is not documented inside the contract, or the response body doesn't match the one documented in the contract.
Either way, these also must be fixed as it shows that the OpenAPI spec is incomplete or that the actual implementation deviated from it.

With a small effort investment we were able to "write", run and report 430 test cases which otherwise would have taken significant more effort to implement and run.

## Running custom test cases

As mentioned previously `CATs` also supports running custom tests with minimal effort. This is done using the [CustomFuzzer](https://github.com/Endava/cats#customfuzzer).
The `CustomFuzzer` run tests configured within a `customFuzzer` file that has a straightforward syntax.
Below is an example of running 2 simple test cases that create a `Pet` and verify the details of a `Pet`. This is saved as `customFuzzer-pet.yml`.
Ideally we should correlate these 2, but it seems that the pet-store API is not supporting it.

```yml
/pet:
  test1:
    description: Create a new Pet
    name: "CATs"
    expectedResponseCode: 200
/pet/{petId}:
  test2:
    description: Get the details of a given Pet
    petId: 11
    expectedResponseCode: 200
    verify:
      id: 11
      category#id: 7
```

We now run `CATs` again:

```bash
./cats.jar  --contract=openapi.yml --server=http://localhost/v3 --headers=header_pet.yml --fuzzers=CustomFuzzer --customFuzzerFile=customFuzzer-pet.yml
```

And we get the following result:

![custom](https://github.com/ludovicianul/ludovicianul.github.io/raw/master/images/custom.png)

The cool think about the custom tests is that you don't need to fill in all the details within the request, just the fields that you care about.
And you get the same benefits as when writing the test cases using actual code:
- you can choose the elements to verify in the responses, using the `verify` section
- you can check for expected HTTP status codes using `expectedResponseCode`
- and you can even pass values from one test to another using `output`

# What about auto-healing
Wel.. auto-healing is built in. As `CATs` generates all its tests from the OpenAPI contract, when the API updates i.e the OpenAPI contract updates also, `CATs` will generate the tests accordingly.

# Conclusions

`CATs` is not intended to replace your existing tooling (although it might do it to a considerable extent). 
It's a simple way to remove some of the boring parts of API testing.
In the same time, it might be appealing for people that want to automate API test cases, but might not have the necessary skills to use tools required significant coding.
Why not [give it a try](https://github.com/Endava/cats)?

